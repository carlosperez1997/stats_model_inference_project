raw_train <- raw_train[raw_train$address_simple %in% selected_addresses, ]
cat('Train size before', dim(raw_test), ': after', dim(raw_test[raw_test$address_simple %in% selected_addresses, ]))
raw_test <- raw_test[raw_test$address_simple %in% selected_addresses, ]
generate_dummies <- function(train, test, column) {
# Combine train and test
combined_data <- rbind(train, test)
# Creating dummy variables (assuming 'category' is your categorical column)
combined_data_dummy <- model.matrix(~ get(column) - 1, combined_data)
col_names <- stringr::str_replace(colnames(combined_data_dummy), "get\\(column\\)", paste0(column, '_'))
#print(col_names)
colnames(combined_data_dummy) <- col_names
# Splitting back into training and testing sets
train_rows <- nrow(train)
x_train_dummy <- combined_data_dummy[1:train_rows, ]
x_test_dummy <- combined_data_dummy[(train_rows + 1):nrow(combined_data_dummy), ]
# Add the dummy variables to the original dataframes
train <- cbind(train, x_train_dummy)
test <- cbind(test, x_test_dummy)
return(list(train = train, test = test))
}
if (dummy_addresses) {
result <- generate_dummies(raw_train, raw_test, 'address_simple')
raw_train <- result$train
raw_test <- result$test
}
obtain_subdistrict <- function(df) {
df$subdistrict <- sapply(strsplit(df$address_simple, ",\\s*"), function(x) x[[2]])
df[df$subdistrict == 'Dubai', 'subdistrict'] <- sapply(strsplit(df[df$subdistrict == 'Dubai', 'address_simple'], ",\\s*"), function(x) x[[1]])
return(df)
}
if (dummy_subdistricts) {
raw_train <- obtain_subdistrict(raw_train)
raw_test <- obtain_subdistrict(raw_test)
result <- generate_dummies(raw_train, raw_test, 'subdistrict')
raw_train <- result$train
raw_test <- result$test
}
do_quantiles <- function(data, quantiles, prefix) {
cuts <- cut(data, quantiles, include.lowest = T)
cuts <- paste0(prefix, cuts)
return(cuts)
}
combined_data <- rbind(raw_train, raw_test)
# Function to assign subregions
assign_subregion <- function(data, lat_breaks, long_breaks) {
data$subregion <- with(data, paste(
cut(lat, breaks = lat_breaks, labels = FALSE, include.lowest = TRUE),
cut(lon, breaks = long_breaks, labels = FALSE, include.lowest = TRUE),
sep = "_"
))
return(data)
}
if (square_cordinates){
# Lat
lat_quantiles <- unique(quantile(combined_data$lat, 0:qcuts/qcuts))
#combined_data$lat_cuts <- do_quantiles(combined_data$lat, quantiles, 'lat_')
# Lon
lon_quantiles <- unique(quantile(combined_data$lon, 0:qcuts/qcuts))
#combined_data$lon_cuts <- do_quantiles(combined_data$lon, quantiles, 'lon_')
# Assign subregions to both datasets
raw_train <- assign_subregion(raw_train, lat_quantiles, lon_quantiles)
raw_test <- assign_subregion(raw_test, lat_quantiles, lon_quantiles)
result <- generate_dummies(raw_train, raw_test, 'subregion')
raw_train <- result$train
raw_test <- result$test
}
if (dummy_coordinates) {
# Lat
quantiles <- unique(quantile(combined_data$lat, 0:qcuts/qcuts))
raw_train$lat_cuts <- do_quantiles(raw_train$lat, quantiles, 'lat_')
raw_test$lat_cuts <- do_quantiles(raw_test$lat, quantiles, 'lat_')
# Lon
quantiles <- unique(quantile(combined_data$lon, 0:qcuts/qcuts))
raw_train$lon_cuts <- do_quantiles(raw_train$lon, quantiles, 'lon_')
raw_test$lon_cuts <- do_quantiles(raw_test$lon, quantiles, 'lon_')
# Lat
result <- generate_dummies(raw_train, raw_test, 'lat_cuts')
raw_train <- result$train
raw_test <- result$test
# Lon
result <- generate_dummies(raw_train, raw_test, 'lon_cuts')
raw_train <- result$train
raw_test <- result$test
}
test <- raw_test
train <-raw_train
# Keep numeric columns
numeric <- sapply(train, is.numeric)
numeric_test <- sapply(train, is.numeric)
train <- train[, numeric]
test <- test[, numeric]
# Remove columns with 0 variance
test= test[,apply(test, 2, var) > 0] #remove columns with 0 variance
train= train[,apply(train, 2, var) > 0] #remove columns with 0 variance
cat(dim(train), dim(test))
# To have same columns in train and test
train <- train[, intersect(names(train), names(test))]
test <- test[, intersect(names(train), names(test))]
cat(dim(train), dim(test))
# Define create interactions terms function
create_interaction_terms <- function(data, variables) {
# Identify columns that start with "community_"
subdistrict_cols <- grep("^community_", names(data), value = TRUE)
# Loop over these columns and create interaction terms
for(col in subdistrict_cols) {
for(var in variables) {
# Check if the variable exists in the data
if(var %in% names(data)) {
# Create interaction term and add it to the data
data[paste0(col, "_", var)] <- data[, col] * data[, var]
} else {
warning(paste("Variable", var, "not found in dataframe. Skipping."))
}
}
}
# Return the modified dataframe
return(data)
}
# Define the list of variables for interaction
variables_to_interact <- c("beds", "area", "baths", "balcony", "parking", "swimming_pool", "furnished")
# Call the function with your dataframe and the list of variables
train <- create_interaction_terms(train, variables_to_interact)
test <- create_interaction_terms(test, variables_to_interact)
train$area_sq <- train$area^2
test$area_sq <- test$area^2
train$bed_sq <- train$bed^2
test$bed_sq <- test$bed^2
train$baths_sq <- train$baths^2
test$baths_sq <- test$baths^2
# Check for correlation
correlation_matrix <- cor(train)
high_correlation <- which(abs(correlation_matrix) > 0.99, arr.ind = TRUE)
# Return which variables are getting removex
high_correlation <- high_correlation[high_correlation[, 1] < high_correlation[, 2], ]
pairs_to_remove <- apply(high_correlation, 1, function(index) {
pair <- colnames(train)[index]
#cat("High correlation between:", pair[1], "and", pair[2], "\n")
#cat("Removing:", pair[2], "\n\n")
return(pair[2])
})
# Remove
test <- test[, !colnames(test) %in% pairs_to_remove]
train <- train[, !colnames(train) %in% pairs_to_remove]
# Split into X and Y for scaling
x_train <- select(train, -price)
num_scale <- 1 # to make weights more interpretable
y_train_orig <- train$price/num_scale
if (log_price) {
y_train <- log(train$price/num_scale)
} else{
y_train <- train$price/num_scale
}
x_test <- select(test, -price)
y_test_orig <- test$price/num_scale
if (log_price) {
y_test <- log(test$price/num_scale)
} else{
y_test <- test$price/num_scale
}
# Save unscaled
x_train_unscaled <- x_train
x_test_unscaled <- x_test
# Scale
x_train <- scale(x_train)
x_test <- scale(x_test)
# Create dfs
y_train_df <- data.frame(y_train)
x_train_df <- data.frame(x_train)
colnames(y_train_df)<-'price'
y_test_df <- data.frame(y_test)
x_test_df <- data.frame(x_test)
colnames(y_test_df)<-'price'
# Drop same price variables
if (drop_prev_prices) {
# save data frame with price vars
x_test_df_with_pricevars <- x_test_df
x_train_df_with_pricevars <- x_train_df
# Drop from main dfs
x_test_df <- x_test_df %>% select(-contains("same"))
x_train_df <- x_train_df %>% select(-contains("same"))
}
train_df<-cbind(x_train_df,y_train_df)
test_df<-cbind(x_test_df,y_test_df)
# Regression metrics
rmse <- function(observed, predicted) {
sqrt(mean((observed - predicted) ^ 2))
}
log_rmse <- function(observed, predicted) {
sqrt(mean((log(observed) - log(predicted)) ^ 2))
}
mape <- function(observed, predicted) {
mean(abs((predicted - observed) / observed)) * 100
}
mae <- function(observed, predicted) {
mean(abs(observed - predicted))
}
# Metrics CV
regression_metrics <- function(model, data, y_real, num_scale, y_wout_log) {
pred <- predict(model, newx = as.matrix(data), s = fit.lasso$lambda.min, type = "response")
if (log_price) {
pred <- exp(pred)
y_real <- y_wout_log
}
rmse_value <- rmse(y_real, pred)*num_scale
mape <- mape(y_real, pred)
mae <- mae(y_real, pred)*num_scale
cat('\nRMSE:', rmse_value, 'MAPE:', mape, 'MAE:', mae)
return(list(rmse=rmse_value, mape=mape, mae=mae))
}
# Calculate statistics for y_train
n_obs_train <- length(y_train)
mean_train <- mean(exp(y_train))
quartiles_train <- quantile(as.matrix(exp(y_train)), probs = c(0.25, 0.5, 0.75))
# Calculate statistics for y_test
n_obs_test <- length(y_test)
mean_test <- mean(y_test)
quartiles_test <- quantile(as.matrix(y_test), probs = c(0.25, 0.5, 0.75))
# Create the data frame with the statistics
stats_table <- data.frame(
Dataset = c("y_train", "y_test"),
Observations = c(n_obs_train, n_obs_test),
Mean = c(mean_train, mean_test),
`1st Quartile` = c(quartiles_train[1], quartiles_test[1]),
Median = c(quartiles_train[2], quartiles_test[2]),
`3rd Quartile` = c(quartiles_train[3], quartiles_test[3])
)
stats_table_long <- stats_table %>%
pivot_longer(cols = -Dataset, names_to = "Metric", values_to = "Value")
# Spread the long format table to wide format with one column for y_train and one for y_test
stats_table_wide <- stats_table_long %>%
pivot_wider(names_from = Dataset, values_from = Value)
# Create a rotated LaTeX formatted table
output_stats <- 0
if (output_stats == 1) {
kable(stats_table_wide, format = "latex", booktabs = TRUE, caption = "Descriptive Statistics of y_train and y_test", row.names = FALSE) %>%
kable_styling(full_width = FALSE, position = "center", latex_options = "striped") %>%
row_spec(0, bold = TRUE, color = "white", background = "#56B4E9")
}
x_test_df[is.na(x_test_df)] <- 0
x_train_df[is.na(x_train_df)] <- 0
cat(dim(x_train_df))
lasso.bic <- function(y,x,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- n * log(colSums((y-pred)^2)/length(y)) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- n * log(colSums((y-pred)^2)/length(y)) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
# Fit model
fit.lasso_ebic = lasso.bic(x=as.matrix(x_train_df), y=y_train ,extended = TRUE)
fit.lasso_ebic
# Lambda
fit.lasso_ebic$lambda.opt
# Extract coefficients
coef_lasso_ebic <- fit.lasso_ebic$coef
coef_matrix_ebic <- as.matrix(coef_lasso_ebic*100)
# Output non zero coefficients
non_zero <- coef_matrix_ebic !=0
coef_table_ebic <- data.frame(Variable = rownames(coef_matrix_ebic)[non_zero], Coefficients = coef_matrix_ebic[non_zero])
coef_table_ebic <- coef_table_ebic[order(-abs(coef_table_ebic$Coefficients)),]
coef_table_ebic$Coefficients <- round(coef_table_ebic$Coefficients, digits = 4)
coef_table_ebic$Coefficients <- format(coef_table_ebic$Coefficients, scientific = FALSE)
coef_table_ebic_output <- coef_table_ebic[0:20,]
coef_table_ebic_output
# Output to Latex
output_lasso_ebic <- F
if (output_lasso_ebic) {
kable(coef_table_ebic_output, format = "latex", booktabs = TRUE, caption = "LASSO EBIC Regression results", row.names = FALSE) %>%
kable_styling(full_width = FALSE, position = "center") %>%
column_spec(1, border_right = TRUE) %>%
column_spec(2, width = "2em") %>%
row_spec(0, bold = TRUE, color = "white", background = "#56B4E9")
}
# Fit model
fit.lasso = cv.glmnet(x=as.matrix(x_train_df), y=y_train, nfolds=10)
View(coef_table_ebic_output)
fit.lasso
fit.lasso$lambda.min
# TEST: RMSE: 330771.3 MAPE: 14.14537 MAE: 186598.
q <- regression_metrics(fit.lasso, x_test_df, y_test, num_scale, y_test_orig)
# TRAIN: RMSE: 222874.8 MAPE: 10.37756 MAE: 142418.3
q <- regression_metrics(fit.lasso, x_train_df, y_train, num_scale, y_train_orig)
# Extract coefficients
coef_lasso <- predict(fit.lasso, type = "coefficients", s = fit.lasso$lambda.min)
coef_matrix <- as.matrix(coef_lasso)
# Output non zero coefficients
non_zero <- coef_matrix !=0
coef_table <- data.frame(Variable = rownames(coef_matrix)[non_zero], Coefficients = coef_matrix[non_zero])
coef_table <- coef_table[order(-abs(coef_table$Coefficients)),]
coef_table$Coefficients <- round(coef_table$Coefficients, digits = 4)
coef_table$Coefficients <- format(coef_table$Coefficients, scientific = FALSE)
coef_table_output <- coef_table[0:20,]
coef_table_output
# Group prefixes for regression output
coef_table <- coef_table %>%
mutate(prefix = case_when(
str_detect(Variable, "address_simple_") ~ "Address indicators",
str_detect(Variable, "AED_") ~ "AED exchange rates",
str_detect(Variable, "comm_") ~ "Comm variables",
str_detect(Variable, "community_") ~ "Community indicators",
str_detect(Variable, "emirate_") ~ "Emirate variables",
str_detect(Variable, "area_sq") ~ "Area squared",
str_detect(Variable, "area") ~ "Area",
str_detect(Variable, "balcony") ~ "# Balconys",
str_detect(Variable, "baths_sq") ~ "# Balconys squared",
str_detect(Variable, "bed_sq") ~ "# Beds squared",
str_detect(Variable, "bed") ~ "# Beds",
str_detect(Variable, "furnished") ~ "# Furnished indicator",
str_detect(Variable, "gold") ~ "Gold price",
str_detect(Variable, "metro") ~ "Distance to metro",
str_detect(Variable, "parking") ~ "Parking indicator",
str_detect(Variable, "lon") ~ "Longitude",
str_detect(Variable, "gdp") ~ "GDP",
str_detect(Variable, "hospital") ~ "Distance to hospital",
str_detect(Variable, "subregion") ~ "Region indicators (by geographic coordinates)",
str_detect(Variable, "market") ~ "Distance to grocery store",
str_detect(Variable, "parking") ~ "Parking indicator",
str_detect(Variable, "subdistrict") ~ "Subdistrict indicators",
str_detect(Variable, "swimming_pool") ~ "Swimming pool indicator",
str_detect(Variable, "USD_BTC") ~ "USC-crypto exchange rate",
str_detect(Variable, "(Intercept)") ~ "(Intercept)",
TRUE ~ "Misc" # For all other cases
))
# Collapse the dataframe to only keep the first instance of each prefix
# and count the number of variables of each type
coef_table$Coefficients <- as.numeric(coef_table$Coefficients)
collapsed_coef_matrix <- coef_table %>%
group_by(prefix) %>%
summarise(Mean = mean(100*Coefficients),
Min = min(100*Coefficients),
Max = max(100*Coefficients),
Count = n()) %>%
mutate(Mean = round(Mean, 2),
Min = round(Min, 2),
Max = round(Max, 2)) %>%
arrange(desc(Count))  # Sorting by Max in descending order
# Output to Latex
output_lasso_2 <- F
if (output_lasso_2) {
kable(collapsed_coef_matrix, format = "latex", booktabs = TRUE, caption = "LASSO Regression results", row.names = FALSE) %>%
kable_styling(full_width = FALSE, position = "center") %>%
column_spec(1, border_right = TRUE) %>%
column_spec(2, width = "2em") %>%
row_spec(0, bold = TRUE, color = "white", background = "#56B4E9")
}
# Average percentage error
pred <- predict(fit.lasso, newx = as.matrix(x_test_df), s = fit.lasso$lambda.min, type = "response")
if (log_price) {
pred <- exp(pred)
y_test <- y_test_orig
}
compare_df <- data.frame(y_test = y_test, predictions = pred)
compare_df$perc_error <- abs((compare_df$y_test-compare_df$s1)/compare_df$y_test)*100
compare_df$error <- abs(compare_df$s1 -compare_df$y_test)
# Calculate the deciles
deciles <- quantile(compare_df$perc_error, probs = seq(0, 1, by = 0.1), na.rm = TRUE)
print(deciles)
# Prepare data for graphs
test_graph <- data.frame(x_test_unscaled)
test_graph$perc_error <- compare_df$perc_error
test_graph$error  <- compare_df$error
test_graph$price <- compare_df$y_test
test_graph
# Graph abs error
plot_abs_error<- ggplot(test_graph, aes(x = price, y = error)) +
theme_minimal() +
geom_point(color = '#1f77b4' ,size = 0.5) +  # Scatter plot points
xlab("Price") +
ylab("Absolute Error") +
theme(legend.position="none",
text = element_text(size = 16),  # Adjust the font size here
axis.title = element_text(size = 16),  # Adjust the title font size
axis.text = element_text(size = 10)) +  # Adjust the axis text font size
scale_x_continuous(labels = scales::comma)
plot_abs_error
# Graph prediction error
#Prepare data
test_graph_dt <- test_graph %>% as.data.table
test_graph_dt[, community := 'Dubai Marina']
test_graph_dt<- test_graph_dt %>%
.[community_Jumeirah.Beach.Residence..JBR.==T, community := 'Jumeirah Beach \n Residence (JBR)'] %>%
.[community_Jumeirah.Lake.Towers..JLT.==T, community := 'Jumeirah Lake \n Towers (JLT)'] %>%
.[community_Jumeirah.Village.Circle..JVC.==T, community := 'Jumeirah Village \n Circle (JVC)']
plot_perc_error_community <- ggplot(test_graph_dt[test_graph_dt$perc_error < 100],
aes(y=community, x=perc_error, color=community)) +
geom_violin(aes(fill = community,
fill = after_scale(colorspace::lighten(fill, .2))),
draw_quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
size = 0.2, scale='width', colour = "black") +
stat_summary(fun=mean, geom="point", shape=16, size=3, color="black") + # Highlight the mean in red
theme_minimal() +
theme(legend.position="none",
text = element_text(size = 16),
axis.title = element_text(size = 16),
axis.text = element_text(size = 10),
axis.ticks = element_line(size = 0.8),
axis.title.y.left=element_text(color="black", size=14),
axis.text.y.left=element_text(color="black", size=12),
axis.title.y.right=element_text(color="black", size=14),
axis.text.y.right=element_text(color="black", size=14),
axis.text.x=element_text(size=12),
axis.title.x=element_text(size=16),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold")) +
scale_x_continuous(labels = scales::comma) +
labs(y='', x="Prediction Error (%)")
plot_perc_error_community
# Graph percentage error
plot_perc_error<- ggplot(test_graph, aes(x = price, y = perc_error)) +
theme_minimal() +
geom_point(color = '#1f77b4' ,size = 0.5) +  # Scatter plot points
xlab("Price") +
ylab("Percentage Error") +
theme(legend.position="none",
text = element_text(size = 16),  # Adjust the font size here
axis.title = element_text(size = 16),  # Adjust the title font size
axis.text = element_text(size = 10)) +  # Adjust the axis text font size
scale_x_continuous(labels = scales::comma)
plot_perc_error
test_graph$community_var <- test_graph_dt$community
dubai_marina_data <- test_graph %>%
filter(community_var == 'Dubai Marina')
# Graph percentage error for Dubai Marina
plot_perc_error<- ggplot(dubai_marina_data, aes(x = price, y = perc_error)) +
theme_minimal() +
geom_point(color = '#1f77b4' ,size = 0.5) +  # Scatter plot points
xlab("Price") +
ylab("Percentage Error") +
theme(legend.position="none",
text = element_text(size = 16),  # Adjust the font size here
axis.title = element_text(size = 16),  # Adjust the title font size
axis.text = element_text(size = 10)) +  # Adjust the axis text font size
scale_x_continuous(labels = scales::comma)
plot_perc_error
# Summary table
summary_table <- test_graph_dt %>%
group_by(community) %>%
summarise(
Q1 = paste0(round(quantile(perc_error, 0.25, na.rm = TRUE), 2), "%"),
Median = paste0(round(median(perc_error, na.rm = TRUE) , 2), "%"),
Mean = paste0(round(mean(perc_error, na.rm = TRUE), 2), "%"),
Q3 = paste0(round(quantile(perc_error, 0.75, na.rm = TRUE), 2), "%"),
)
print(summary_table)
summary_tab_output<-F
if (summary_tab_output) {
kable(summary_table, "latex", booktabs = TRUE, caption = "Summary Table of perc_error by community") %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 1, "Statistics" = 5)) %>%
column_spec(1, bold = TRUE, color = "white", background = "#56B4E9") %>%
column_spec(2:6, background = "#F0F0F0")
}
# Get community variable
train_dt <- raw_train %>% as.data.table
train_dt[, community := 'Dubai Marina']
train_dt<- train_dt %>%
.[community_Jumeirah.Beach.Residence..JBR.==T, community := 'Jumeirah Beach \n Residence (JBR)'] %>%
.[community_Jumeirah.Lake.Towers..JLT.==T, community := 'Jumeirah Lake \n Towers (JLT)'] %>%
.[community_Jumeirah.Village.Circle..JVC.==T, community := 'Jumeirah Village \n Circle (JVC)']
# Understand high prediction errors
investigate_train <-cbind(exp(y_train_df), address = raw_train$address_simple, subdistrict =raw_train$subdistrict, community = train_dt$community)
investigate_test <-cbind(exp(y_test_df), prediction = pred, test_graph_dt$perc_error, address = raw_test$address_simple, subdistrict =raw_test$subdistrict, community = test_graph_dt$community)
# Caculate mean values by region in train
variables <- c("price")
investigate_train <- investigate_train %>%
group_by(address) %>%
mutate(across(all_of(variables), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}_address")) %>%
ungroup() %>%
group_by(subdistrict) %>%
mutate(across(all_of(variables), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}_sub")) %>%
ungroup() %>%
group_by(community) %>%
mutate(across(all_of(variables), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}_com")) %>%
ungroup()
# Drop duplicates
investigate_train <- select(investigate_train, -variables)
investigate_train <- investigate_train %>%
distinct()
# Merge with test
merged_df <- merge(investigate_test, investigate_train, by.x=c("address", "subdistrict", "community"), by.y=c("address", "subdistrict", "community"), all.x=TRUE)
merged_df$diff_add <- ((merged_df$price - merged_df$mean_price_address)/merged_df$mean_price_address)*100
merged_df$diff_region <- ((merged_df$price - merged_df$mean_price_sub)/merged_df$mean_price_sub)*100
merged_df$diff_com <- ((merged_df$price - merged_df$mean_price_com)/merged_df$mean_price_com)*100
# Investigatemax error observation
understand <- cbind(perc = test_graph_dt$perc_error, x_test_df)
max_error_row <- understand[which.max(understand$perc), ]
# First, transpose 'max_error_row' and convert it to a dataframe
max_error_row_t <- as.data.frame(t(max_error_row))
# Column-bind the transposed max_error_row with coef_lasso
understand_test <- cbind(coef_matrix, max_error_row_t)
understand_test$output <- understand_test$s1 * understand_test$`3179`
source("~/Documents/MASTERS/Statistical Modeling and Inference/Project/Project git hub/regression - to submit.Rmd")
source("~/Documents/MASTERS/Statistical Modeling and Inference/Project/Project git hub/regression - to submit.Rmd")
